{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7eea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33c6c741",
   "metadata": {},
   "source": [
    "Create a basic shell script\n",
    "\n",
    "\n",
    "Here we will define a shell script extract_transform_load.sh which will define a pipeline of tasks such as\n",
    "\n",
    "-extract\n",
    "-transform\n",
    "-load\n",
    "For now, let the shell script have basic echo statements for extract,transform and load.\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Extract data\"\n",
    "\n",
    "echo \"Transform data\"\n",
    "\n",
    "echo \"Load data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b4a6e",
   "metadata": {},
   "source": [
    "An Apache Airflow DAG is a python program. It consists of these logical blocks.\n",
    "\n",
    "Imports\n",
    "DAG Arguments\n",
    "DAG Definition\n",
    "Task Definitions\n",
    "Task Pipeline\n",
    "A typical imports block looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb315e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ef9ea",
   "metadata": {},
   "source": [
    "DAG arguments are like settings for the DAG.\n",
    "\n",
    "The above settings mention\n",
    "\n",
    "the owner name,\n",
    "when this DAG should run from: days_age(0) means today,\n",
    "the email address where the alerts are sent to,\n",
    "whether alert must be sent on failure,\n",
    "whether alert must be sent on retry,\n",
    "the number of retries in case of failure, and\n",
    "the time delay between retries.\n",
    "A typical DAG definition block looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the DAG\n",
    "dag = DAG(\n",
    "    dag_id='sample-etl-dag',\n",
    "    default_args=default_args,\n",
    "    description='Sample ETL DAG using Bash',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad75a3d",
   "metadata": {},
   "source": [
    "Here we are creating a variable named -dag- by instantiating the DAG class with the following parameters.\n",
    "\n",
    "-sample-etl-dag- is the ID of the DAG. This is what you see on the web console.\n",
    "\n",
    "We are passing the dictionary -default_args-, in which all the defaults are defined.\n",
    "\n",
    "-description- helps us in understanding what this DAG does.\n",
    "\n",
    "-schedule_interval- tells us how frequently this DAG runs. In this case every day. (days=1).\n",
    "\n",
    "A typical -task definitions- block looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tasks\n",
    "\n",
    "# define the task named extract_transform_and_load to call the shell script\n",
    "extract_transform_and_load = BashOperator(\n",
    "    task_id='extract_transform_and_load',\n",
    "    bash_command='/home/project/airflow/dags/extract_transform_load.sh \"',\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b92485",
   "metadata": {},
   "source": [
    "A task is defined using:\n",
    "\n",
    "A task_id which is a string and helps in identifying the task.\n",
    "What bash command it represents. Here we are calling the shell script extract_transform_load.shwhich we previously defined\n",
    "Which dag this task belongs to.\n",
    "A typical task pipeline block looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task pipeline\n",
    "extract_transform_and_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413d753",
   "metadata": {},
   "source": [
    "# Submit a DAG\n",
    "Submitting a DAG is as simple as copying the DAG python file into dags folder in the AIRFLOW_HOME directory.\n",
    "\n",
    "Open a terminal and run the command below to submit the DAG that was created in the previous exercise.\n",
    "\n",
    "Note: While submitting the dag that was created in the previous exercise, use sudo in the terminal before the command used to submit the dag.\n",
    "\n",
    " cp my_first_dag.py $AIRFLOW_HOME/dags\n",
    " \n",
    "# Next, run the command below one by one to submit shell script in the dags folder and to change the permission for reading shell script.\n",
    "\n",
    " cp my_first_dag.sh $AIRFLOW_HOME/dags\n",
    " cd airflow/dags\n",
    " chmod 777 my_first_dag.sh\n",
    " \n",
    "# Verify that our DAG actually got submitted.\n",
    "\n",
    "# Run the command below to list out all the existing DAGs\n",
    "\n",
    "airflow dags list\n",
    "\n",
    "#  Verify that my-first-dag is a part of the output.\n",
    "\n",
    "airflow dags list|grep \"my-first-dag\"\n",
    "\n",
    "# You should see your DAG name in the output.\n",
    "\n",
    "Run the command below to list out all the tasks in my-first-dag.\n",
    "\n",
    "airflow tasks list my-first-dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b70b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370ce4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
