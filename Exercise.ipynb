{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1ec39b",
   "metadata": {},
   "source": [
    "Problem:\n",
    "Download the dataset from the source to the destination mentioned below using wget command in terminal.\n",
    "\n",
    "Note: While downloading the file in the terminal use the sudo command before the command used to download the file.\n",
    "\n",
    "Source : https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\n",
    "\n",
    "Destination : /home/project/airflow/dags.\n",
    "\n",
    "The server access log file contains these fields.\n",
    "\n",
    "a. timestamp - TIMESTAMP\n",
    "\n",
    "b. latitude - float\n",
    "\n",
    "c. longitude - float\n",
    "\n",
    "d. visitorid - char(37)\n",
    "\n",
    "e. accessed_from_mobile - boolean\n",
    "\n",
    "f. browser_code - int\n",
    "\n",
    "Write a shell script named ETL_Server_Access_Log_Processing.sh.\n",
    "\n",
    "Task 1: Add a command in the shell script to extract the fields timestamp and visitorid from the web-server-access-log.txt and write to a file extracted.txt\n",
    "\n",
    "Task 2: Update the shell script to add a command to capitalize the visitorid and write to a new file capitalized.txt.\n",
    "\n",
    "Task 3: Update the shell script to add a command to compress the transformed file capitalized.txt into a new file log.tar.gz.\n",
    "\n",
    "Write a DAG named ETL_Server_Access_Log_Processing.\n",
    "\n",
    "Task 2: Create the imports block.\n",
    "\n",
    "Task 3: Create the DAG Arguments block. You can use the default settings\n",
    "\n",
    "Task 4: Create the DAG definition block. The DAG should run daily.\n",
    "\n",
    "Task 5: Create the task extract_transform_and_load to call the shell script.\n",
    "\n",
    "Task 6: Create the task pipeline block.\n",
    "\n",
    "Task 7: Submit the DAG.\n",
    "\n",
    "Task 8: Submit the shell script to dags folder.\n",
    "\n",
    "Task 9: Change the permission to read shell script.\n",
    "\n",
    "Task 10. Verify if the DAG is submitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f4b32",
   "metadata": {},
   "source": [
    "Download the dataset from the source to the destination mentioned below using wget command in terminal.\n",
    "\n",
    "Note: While downloading the file in the terminal use the sudo command before the command used to download the file.\n",
    "\n",
    "Source : https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\n",
    "\n",
    "Destination : /home/project/airflow/dags\n",
    "\n",
    "Select File -> New File from the menu and name it as ETL_Server_Access_Log_Processing.sh.\n",
    "\n",
    "Copy the code below in the shell script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Create a shell script having the following commands.\n",
    "#!/bin/bash\n",
    "echo \"extract_transform_load\"\n",
    "# cut command to extract the fields timestamp and visitorid writes to a new file extracted.txt\n",
    "cut -f1,4 -d\"#\" /home/project/airflow/dags/web-server-access-log.txt > /home/project/airflow/dags/extracted.txt\n",
    "\n",
    "# tr command to transform by capitalizing the visitorid.\n",
    "tr \"[a-z]\" \"[A-Z]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt\n",
    "\n",
    "# tar command to compress the extracted and transformed data.\n",
    "tar -czvf /home/project/airflow/dags/log.tar.gz /home/project/airflow/dags/capitalized.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17710a1d",
   "metadata": {},
   "source": [
    "Next select File -> New File from the menu and name it as \n",
    "\n",
    "ETL_Server_Access_Log_Processing.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Create the imports block.\n",
    "# import the libraries\n",
    "\n",
    "from datetime import timedelta\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "# Operators; we need this to write tasks!\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "# This makes scheduling easy\n",
    "from airflow.utils.dates import days_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG Arguments block. You can use the default settings.\n",
    "#defining DAG arguments\n",
    "\n",
    "# You can override them on a per-task basis during operator initialization\n",
    "default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5453d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG definition block. The DAG should run daily.\n",
    "# defining the DAG\n",
    "\n",
    "# define the DAG\n",
    "dag = DAG(\n",
    "    'ETL_Server_Access_Log_Processing',\n",
    "    default_args=default_args,\n",
    "    description='My first DAG',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef1123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the task extract_transform_and_load to call the shell script.\n",
    "# define the tasks\n",
    "\n",
    "#define the task named extract_transform_and_load to call the shell script\n",
    "#calling the shell script\n",
    "extract_transform_and_load = BashOperator(\n",
    "    task_id=\"extract_transform_and_load\",\n",
    "    bash_command=\"/home/project/airflow/dags/ETL_Server_Access_Log_Processing.sh \",\n",
    "    dag=dag,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the task pipeline block.\n",
    "# task pipeline\n",
    "\n",
    "extract_transform_and_load\n",
    "# save the python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85346e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the DAG.\n",
    "cp  ETL_Server_Access_Log_Processing.py $AIRFLOW_HOME/dags\n",
    "\n",
    "# Submit the shell script to dags folder.\n",
    "cp  ETL_Server_Access_Log_Processing.sh $AIRFLOW_HOME/dags\n",
    "\n",
    "# Change the permission to read shell script.\n",
    "cd airflow/dags\n",
    "chmod 777 ETL_Server_Access_Log_Processing.sh\n",
    "\n",
    "# Verify if the DAG is submitted.\n",
    "airflow dags list\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
